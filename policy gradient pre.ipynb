{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc69b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Define the policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_features, num_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.softmax(x, dim=-1)\n",
    "\n",
    "# Constants\n",
    "num_features = ...  # Define the size of the observation space\n",
    "num_actions = ...  # Define the total number of actions\n",
    "\n",
    "# Create the policy network\n",
    "policy = PolicyNetwork(num_features, num_actions)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "# Function to compute discounted rewards\n",
    "def discount_rewards(rewards, gamma):\n",
    "    ...\n",
    "\n",
    "# Training function\n",
    "def train_policy_gradient(policy, optimizer, states, actions, rewards):\n",
    "    discounted_rewards = discount_rewards(rewards, gamma)\n",
    "    \n",
    "    policy_loss = []\n",
    "    for state, action, reward in zip(states, actions, discounted_rewards):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        probs = policy(state)\n",
    "        m = Categorical(probs)\n",
    "        loss = -m.log_prob(action) * reward\n",
    "        policy_loss.append(loss)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Assume we have an environment simulator with the following interface\n",
    "class EnvironmentSimulator:\n",
    "    def reset(self):\n",
    "        ...\n",
    "    \n",
    "    def step(self, action):\n",
    "        ...\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 1000\n",
    "gamma = 0.99\n",
    "\n",
    "env_simulator = EnvironmentSimulator()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env_simulator.reset()\n",
    "    states, actions, rewards = [], [], []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs = policy(state_tensor)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        next_state, reward, done = env_simulator.step(action.item())\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "    train_policy_gradient(policy, optimizer, states, actions, rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee5b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Constants\n",
    "num_systems = 4  # This can be dynamic based on the systems you want to operate\n",
    "num_continuous_controls = 5  # Number of variables with continuous control (+-0.1)\n",
    "num_discrete_controls = 3  # Number of variables with discrete control (natural numbers)\n",
    "num_features = ...  #Number of features from the environment table, which would be the number of columns minus static data like 'MM-DD hour' and 'System Identifier'\n",
    "\n",
    "num_continuous_actions_per_control = 3 #Assuming each continuous control can either stay the same, increase, or decrease\n",
    "num_chiller_states = ... # The number of states for the chiller (e.g., off, 1, 2, ...)\n",
    "num_heat_exchanger_states = 2 # The number of states for the heat exchanger\n",
    "num_cooling_towers = 4 # Number of cooling towers you can operate\n",
    "\n",
    "num_actions = (num_continuous_controls * num_continuous_actions_per_control) + num_chiller_states + num_heat_exchanger_states + num_cooling_towers\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99 # Discount factor for rewards\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def init(self, num_features, num_actions):\n",
    "        super(PolicyNetwork, self).init()\n",
    "        self.fc1 = nn.Linear(num_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_actions)\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.softmax(x, dim=-1)\n",
    "    policy = PolicyNetwork(num_features, num_actions)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "def discount_rewards(rewards, gamma):\n",
    "# Function implementation to calculate discounted rewards\n",
    "...\n",
    "\n",
    "def train_policy_gradient(policy, optimizer, states, actions, rewards):\n",
    "discounted_rewards = discount_rewards(rewards, gamma)\n",
    "policy_loss = []\n",
    "for state, action, reward in zip(states, actions, discounted_rewards):\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    loss = -m.log_prob(action) * reward\n",
    "    policy_loss.append(loss)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "policy_loss = torch.stack(policy_loss).sum()\n",
    "policy_loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "def simulate_environment(systems_to_operate, actions):\n",
    "    # Function to apply actions to systems and return new state and reward\n",
    "    ...\n",
    "\n",
    "#train\n",
    "num_episodes = 1000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "# Retrieve current state of the environment\n",
    "    current_state = ... # Current state for all systems\n",
    "    states, actions, rewards = [], [], []\n",
    "    done = False\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.from_numpy(current_state).float().unsqueeze(0)\n",
    "        probs = policy(state_tensor)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "\n",
    "        states.append(current_state)\n",
    "        actions.append(action)\n",
    "\n",
    "        # Apply the action to the environment and retrieve next state and reward\n",
    "        next_state, reward, done = simulate_environment(num_systems, action)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        current_state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "train_policy_gradient(policy, optimizer, states, actions, rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Constants\n",
    "num_systems = 4  # We have 4 identical systems\n",
    "num_continuous_controls = 5  # Variables that can be controlled continuously\n",
    "num_continuous_actions_per_control = 3  # Each can stay the same, increase, or decrease\n",
    "\n",
    "# num_discrete_controls = 1  # Chiller can only be turned on or off\n",
    "# num_towers_per_system = 4  # Each system has 4 towers that can be turned on or off\n",
    "# num_plate_exchanger_states = 2  # The plate exchanger can be on or off\n",
    "\n",
    "# # Since there are 4 towers with on/off states, we calculate the total discrete action space for the towers\n",
    "# num_tower_actions = 2 ** num_towers_per_system  # Each tower can be either on or off\n",
    "\n",
    "# # Total number of actions for the continuous controls\n",
    "# total_continuous_actions = num_continuous_controls * num_continuous_actions_per_control\n",
    "\n",
    "# # Total number of actions for the discrete controls\n",
    "# total_discrete_actions = num_discrete_controls + num_tower_actions + num_plate_exchanger_states\n",
    "\n",
    "\n",
    "num_chillers_per_system = 1  # One chiller per system, with on/off states\n",
    "num_towers_per_system = 4  # Four towers per system, with on/off states for each\n",
    "num_plate_exchangers_per_system = 1  # One plate exchanger per system, with on/off states\n",
    "\n",
    "# Each chiller, tower, and plate exchanger can be on or off, so there are 2 states for each\n",
    "num_discrete_chiller_actions = 2 ** (num_chillers_per_system * num_systems)\n",
    "num_discrete_tower_actions = 2 ** (num_towers_per_system * num_systems)\n",
    "num_discrete_exchanger_actions = 2 ** (num_plate_exchangers_per_system * num_systems)\n",
    "\n",
    "# Summing all discrete actions\n",
    "num_discrete_actions = num_discrete_chiller_actions + num_discrete_tower_actions + num_discrete_exchanger_actions\n",
    "\n",
    "\n",
    "num_actions = num_discrete_actions + num_continuous_actions\n",
    "\n",
    "num_features = ...  # Number of observation features from the environment state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99  # Discount factor for rewards\n",
    "\n",
    "# Policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_features, num_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.softmax(self.fc3(x), dim=-1)\n",
    "\n",
    "# Instantiate the policy network and the optimizer\n",
    "policy = PolicyNetwork(num_features, num_actions)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function to calculate discounted rewards\n",
    "def discount_rewards(rewards, gamma):\n",
    "    # Implementation goes here\n",
    "    ...\n",
    "\n",
    "# Training function for policy gradients\n",
    "def train_policy_gradient(policy, optimizer, states, actions, rewards):\n",
    "    discounted_rewards = discount_rewards(rewards, gamma)\n",
    "    policy_loss = []\n",
    "    for state, action, reward in zip(states, actions, discounted_rewards):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        probs = policy(state)\n",
    "        # We will need to expand this for the multi-action space\n",
    "        action_distribution = Categorical(probs)\n",
    "        loss = -action_distribution.log_prob(action) * reward\n",
    "        policy_loss.append(loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Function to simulate the environment\n",
    "def simulate_environment(systems_to_operate, actions):\n",
    "    # Apply actions to the systems and return the new state and reward\n",
    "    ...\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    # Retrieve the current state of the environment\n",
    "    current_state = ...  # Fetch the current state for all systems from the environment\n",
    "    states, actions, rewards = [], [], []\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.from_numpy(current_state).float().unsqueeze(0)\n",
    "        probs = policy(state_tensor)\n",
    "        # We will need to adapt the action selection to our multi-action space\n",
    "        action_distribution = Categorical(probs)\n",
    "        action = action_distribution.sample()\n",
    "\n",
    "        states.append(current_state)\n",
    "        actions.append(action.item())  # Assuming a single discrete action space\n",
    "\n",
    "        # Apply the action to the environment and retrieve the next state and reward\n",
    "        next_state, reward, done = simulate_environment(num_systems, action.item())\n",
    "\n",
    "        rewards.append(reward)\n",
    "        current_state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    train_policy_gradient(policy, optimizer, states, actions, rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa48d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Normal\n",
    "\n",
    "# Constants\n",
    "num_systems = 4\n",
    "num_continuous_controls = 5\n",
    "\n",
    "num_freature = 23*4\n",
    "\n",
    "# For each system:\n",
    "num_chiller_actions = 2  # On or off\n",
    "num_plate_exchanger_actions = 2  # On or off\n",
    "num_cooling_tower_combinations = 2 ** num_towers_per_system  # Each can be on or off\n",
    "\n",
    "# The action to turn the entire system off (master switch)\n",
    "num_system_off_action = 2\n",
    "\n",
    "# For num_systems systems, the discrete action space is:\n",
    "num_discrete_controls_per_system = num_chiller_actions + num_plate_exchanger_actions + num_cooling_tower_combinations\n",
    "num_discrete_controls = ((num_discrete_controls_per_system+num_continuous_controls+num_system_off_action) * num_systems) \n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99  # Discount factor for rewards\n",
    "\n",
    "# Policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_features, num_continuous_controls, num_discrete_controls):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        # Shared layers\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Branch for continuous controls\n",
    "        self.continuous_head = nn.Sequential(\n",
    "            nn.Linear(64, num_continuous_controls * 2)  # Mean and std dev for each control\n",
    "        )\n",
    "        \n",
    "        # Branch for discrete controls\n",
    "        self.discrete_head = nn.Sequential(\n",
    "            nn.Linear(64, num_discrete_controls)  # Probabilities for discrete actions\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.shared_layers(x)\n",
    "        \n",
    "        # Continuous actions output\n",
    "        continuous_actions = self.continuous_head(x)\n",
    "        means = continuous_actions[:, :num_continuous_controls]\n",
    "        std_devs = torch.clamp(continuous_actions[:, num_continuous_controls:], min=1e-3)\n",
    "        \n",
    "        # Discrete actions output\n",
    "        discrete_logits = self.discrete_head(x)\n",
    "        \n",
    "        return means, std_devs, discrete_logits\n",
    "\n",
    "# Instantiate the policy network and the optimizer\n",
    "policy = PolicyNetwork(num_features, num_continuous_controls, num_discrete_controls)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def discount_rewards(rewards, gamma):\n",
    "    \n",
    "\n",
    "# Training function for policy gradients\n",
    "def train_policy_gradient(policy, optimizer, states, actions, rewards):\n",
    "    discounted_rewards = discount_rewards(rewards, gamma)\n",
    "    policy_loss = []\n",
    "    for state, action, reward in zip(states, actions, discounted_rewards):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        probs = policy(state)\n",
    "        # We will need to expand this for the multi-action space\n",
    "        action_distribution = Categorical(probs)\n",
    "        loss = -action_distribution.log_prob(action) * reward\n",
    "        policy_loss.append(loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "# Assume we have an environment simulator with the following interface\n",
    "class EnvironmentSimulator:\n",
    "    \n",
    "\n",
    "# Training loop\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    # Retrieve the current state of the environment\n",
    "    current_state = ...  # Fetch the current state for all systems from the environment\n",
    "    states, actions, rewards = [], [], []\n",
    "    done = False    \n",
    "    while not done:\n",
    "        state_tensor = torch.from_numpy(current_state).float().unsqueeze(0)\n",
    "        means, std_devs, discrete_logits = policy(state_tensor)\n",
    "        \n",
    "        # Sample from the distributions for continuous actions\n",
    "        continuous_distribution = Normal(means, std_devs)\n",
    "        continuous_actions = continuous_distribution.sample()\n",
    "        \n",
    "        # Sample from the distribution for discrete actions\n",
    "        discrete_distribution = Categorical(logits=discrete_logits)\n",
    "        discrete_actions = discrete_distribution.sample()\n",
    "\n",
    "        # You'll need to process these actions into a valid format for your environment\n",
    "        # ...\n",
    "        states.append(current_state)\n",
    "        actions.append([continuous_actions.item(),discrete_actions.item()])  # Assuming a single discrete action space\n",
    "\n",
    "        # Apply the action to the environment and retrieve the next state and reward\n",
    "        next_state, reward, done = simulate_environment(num_systems, action.item())\n",
    "\n",
    "        rewards.append(reward)\n",
    "        current_state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    train_policy_gradient(policy, optimizer, states, actions, rewards)\n",
    "\n",
    "\n",
    "    \n",
    "for episode in range(num_episodes):\n",
    "    # Retrieve the current state of the real environment\n",
    "    current_state = get_real_system_state()  # This function needs to be implemented to fetch real-world data\n",
    "\n",
    "    # Initialize episode memory\n",
    "    states, actions, rewards = [], [], []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        state_tensor = torch.from_numpy(current_state).float().unsqueeze(0)\n",
    "        means, std_devs, discrete_logits = policy(state_tensor)\n",
    "        \n",
    "        # Sample from the distributions for continuous actions\n",
    "        continuous_distribution = Normal(means, std_devs)\n",
    "        continuous_actions = continuous_distribution.sample()\n",
    "        \n",
    "        # Apply constraints to continuous actions here if necessary\n",
    "        # ...\n",
    "\n",
    "        # Sample from the distribution for discrete actions\n",
    "        discrete_distribution = Categorical(logits=discrete_logits)\n",
    "        discrete_actions = discrete_distribution.sample()\n",
    "\n",
    "        # Combine and possibly constrain the discrete actions here\n",
    "        # ...\n",
    "\n",
    "        # Apply the actions to the real system and get the new state and reward\n",
    "        # Ensure this is done in a safe manner with proper error checking\n",
    "        # next_state, reward = apply_actions_to_real_system(continuous_actions, discrete_actions)\n",
    "        # ...\n",
    "\n",
    "        states.append(current_state)\n",
    "        actions.append((continuous_actions, discrete_actions))  # Store actions taken\n",
    "\n",
    "        # Calculate reward based on the power consumption difference\n",
    "        # reward = calculate_reward_from_power_consumption(current_state, next_state)\n",
    "        # ...\n",
    "\n",
    "        rewards.append(reward)\n",
    "        current_state = next_state\n",
    "\n",
    "        # Implement your stopping criteria\n",
    "        # ...\n",
    "\n",
    "    # Update policy after each episode or after collecting enough data\n",
    "    train_policy_gradient(policy, optimizer, states, actions, rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
